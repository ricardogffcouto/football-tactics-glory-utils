{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street View House Numbers Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Street View House Numbers (SVHN) dataset is one of the most popular benchmarks for object recognition tasks in academic papers. The images were obtained from house numbers in Google Street View images, are hosted by Stanford University and are very similar in philosophy with the MNIST dataset. However, the original purpose of this dataset is to solve a harder problem: that of recognizing digits and numbers in natural scene images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data of the Street View House Numbers dataset, which can originally be found [here](http://ufldl.stanford.edu/housenumbers) are originally in .mat, i.e. files which can be best processed with MATLAB; thus, some preprocessing is required (see section 2). It is important to note that the data are divided into two formats and in this particular kernel we are going to use **Format 2**:\n",
    "\n",
    "- *Format 1*: The original, variable-resolution colored house-number images with character level bounding boxes.\n",
    "- *Format 2*: The cropped digits (32x32 pixels) which follow the philosophy of the MNIST dataset more closely, but also contain some distracting digits to the sides of the digit of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 20:48:56.118045: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 20:48:56.433885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 20:48:56.433902: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-28 20:48:57.427312: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 20:48:57.427374: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 20:48:57.427380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "## 2. Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "train_raw = loadmat('datasets/train_32x32.mat')\n",
    "test_raw = loadmat('datasets/test_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "\n",
    "train_images = np.array(train_raw['X'])\n",
    "test_images = np.array(test_raw['X'])\n",
    "\n",
    "train_labels = train_raw['y']\n",
    "test_labels = test_raw['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73257, 32, 32, 3)\n",
      "(26032, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the data\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 73257, 32, 32)\n",
      "(3, 26032, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Fix the axes of the images\n",
    "\n",
    "train_images = np.moveaxis(train_images, -1, 0)\n",
    "test_images = np.moveaxis(test_images, -1, 0)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test images into 'float64' type\n",
    "\n",
    "train_images = train_images.astype('float64')\n",
    "test_images = test_images.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 33.  15.  15. ...  72.  82.  89.]\n",
      "   [ 28.  14.  14. ...  40.  67.  83.]\n",
      "   [ 40.  18.  16. ...  23.  35.  72.]\n",
      "   ...\n",
      "   [ 86.  87.  82. ... 104. 104. 103.]\n",
      "   [ 84.  86.  82. ... 110. 106. 106.]\n",
      "   [ 85.  89.  85. ... 111. 109. 103.]]\n",
      "\n",
      "  [[ 84.  86.  77. ...  90.  88.  88.]\n",
      "   [ 85.  83.  74. ...  89.  88.  88.]\n",
      "   [ 83.  78.  61. ...  90.  88.  85.]\n",
      "   ...\n",
      "   [100.  98.  95. ... 104. 102. 100.]\n",
      "   [103. 106. 103. ... 103. 103. 105.]\n",
      "   [103. 103. 104. ... 113. 104. 103.]]\n",
      "\n",
      "  [[ 19.  20.  25. ...  65.  78.  98.]\n",
      "   [ 21.  19.  25. ...  63.  91. 130.]\n",
      "   [ 21.  20.  22. ...  79. 125. 178.]\n",
      "   ...\n",
      "   [ 88.  89.  84. ...  62.  67.  74.]\n",
      "   [ 88.  87.  88. ...  60.  61.  65.]\n",
      "   [ 84.  81.  87. ...  63.  62.  63.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 92.  94. 114. ... 200. 192. 190.]\n",
      "   [ 92.  93. 111. ... 181. 177. 183.]\n",
      "   [ 99.  94. 107. ... 167. 172. 185.]\n",
      "   ...\n",
      "   [ 99.  96. 108. ... 210. 206. 203.]\n",
      "   [ 94.  94. 110. ... 219. 218. 212.]\n",
      "   [ 88.  85. 105. ... 217. 221. 218.]]\n",
      "\n",
      "  [[190. 205. 220. ... 229. 229. 229.]\n",
      "   [183. 200. 218. ... 227. 227. 228.]\n",
      "   [171. 189. 213. ... 231. 229. 227.]\n",
      "   ...\n",
      "   [187. 204. 217. ... 204. 196. 196.]\n",
      "   [186. 198. 211. ... 222. 214. 208.]\n",
      "   [190. 199. 208. ... 232. 226. 218.]]\n",
      "\n",
      "  [[216. 221. 226. ... 200. 193. 197.]\n",
      "   [204. 210. 220. ... 201. 195. 196.]\n",
      "   [198. 202. 212. ... 203. 198. 195.]\n",
      "   ...\n",
      "   [233. 230. 228. ... 198. 184. 189.]\n",
      "   [231. 229. 227. ... 195. 181. 186.]\n",
      "   [230. 230. 229. ... 190. 178. 181.]]]\n",
      "\n",
      "\n",
      " [[[ 30.  23.  17. ...  65.  77.  79.]\n",
      "   [ 39.  25.  20. ...  39.  58.  78.]\n",
      "   [ 41.  21.  17. ...  29.  42.  69.]\n",
      "   ...\n",
      "   [ 81.  82.  79. ... 104. 105. 105.]\n",
      "   [ 86.  79.  76. ... 103. 105. 104.]\n",
      "   [ 88.  82.  79. ... 104. 105. 106.]]\n",
      "\n",
      "  [[ 76.  73.  78. ...  78.  77.  78.]\n",
      "   [ 77.  73.  69. ...  82.  79.  81.]\n",
      "   [ 76.  77.  50. ...  85.  83.  82.]\n",
      "   ...\n",
      "   [ 98.  94.  93. ... 104. 102. 101.]\n",
      "   [104. 104. 103. ... 104. 103.  99.]\n",
      "   [106. 105. 106. ... 103. 104.  98.]]\n",
      "\n",
      "  [[ 54.  52.  57. ... 144. 148. 158.]\n",
      "   [ 53.  52.  56. ... 137. 153. 180.]\n",
      "   [ 53.  51.  52. ... 147. 181. 218.]\n",
      "   ...\n",
      "   [162. 163. 156. ... 142. 144. 145.]\n",
      "   [164. 160. 159. ... 141. 141. 143.]\n",
      "   [160. 154. 158. ... 144. 143. 145.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 78.  82. 101. ... 201. 193. 191.]\n",
      "   [ 78.  80.  98. ... 180. 176. 182.]\n",
      "   [ 84.  81.  94. ... 166. 171. 184.]\n",
      "   ...\n",
      "   [ 94.  91. 103. ... 207. 202. 197.]\n",
      "   [ 91.  91. 107. ... 218. 215. 209.]\n",
      "   [ 87.  84. 103. ... 217. 220. 217.]]\n",
      "\n",
      "  [[188. 203. 218. ... 231. 229. 228.]\n",
      "   [182. 199. 217. ... 228. 226. 224.]\n",
      "   [170. 189. 213. ... 230. 226. 223.]\n",
      "   ...\n",
      "   [185. 202. 217. ... 200. 193. 193.]\n",
      "   [184. 197. 211. ... 216. 209. 205.]\n",
      "   [188. 197. 208. ... 227. 221. 213.]]\n",
      "\n",
      "  [[217. 222. 227. ... 199. 188. 189.]\n",
      "   [205. 211. 221. ... 199. 191. 190.]\n",
      "   [198. 202. 211. ... 200. 194. 190.]\n",
      "   ...\n",
      "   [226. 224. 223. ... 196. 183. 187.]\n",
      "   [226. 224. 223. ... 194. 181. 183.]\n",
      "   [226. 226. 225. ... 190. 177. 178.]]]\n",
      "\n",
      "\n",
      " [[[ 38.  19.  19. ...  56.  57.  59.]\n",
      "   [ 35.  22.  17. ...  50.  52.  60.]\n",
      "   [ 38.  26.  23. ...  45.  44.  53.]\n",
      "   ...\n",
      "   [ 75.  71.  65. ...  87.  81.  78.]\n",
      "   [ 64.  72.  72. ...  84.  85.  86.]\n",
      "   [ 68.  72.  67. ...  87.  86.  79.]]\n",
      "\n",
      "  [[ 59.  66.  56. ...  69.  67.  66.]\n",
      "   [ 61.  64.  59. ...  64.  70.  67.]\n",
      "   [ 60.  58.  54. ...  63.  66.  70.]\n",
      "   ...\n",
      "   [ 72.  76.  73. ...  86.  87.  78.]\n",
      "   [ 79.  79.  87. ...  86.  87.  81.]\n",
      "   [ 82.  87.  91. ...  88.  88.  80.]]\n",
      "\n",
      "  [[110. 111. 116. ... 223. 218. 220.]\n",
      "   [110. 106. 111. ... 208. 214. 229.]\n",
      "   [110. 106. 106. ... 210. 230. 254.]\n",
      "   ...\n",
      "   [237. 238. 230. ... 227. 226. 225.]\n",
      "   [240. 237. 237. ... 230. 228. 226.]\n",
      "   [238. 233. 238. ... 235. 232. 231.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[101. 105. 125. ... 203. 195. 193.]\n",
      "   [103. 106. 124. ... 184. 180. 187.]\n",
      "   [112. 110. 123. ... 171. 176. 189.]\n",
      "   ...\n",
      "   [110. 109. 124. ... 204. 200. 199.]\n",
      "   [103. 104. 121. ... 215. 212. 209.]\n",
      "   [ 94.  93. 114. ... 214. 219. 218.]]\n",
      "\n",
      "  [[191. 206. 220. ... 224. 224. 223.]\n",
      "   [186. 201. 218. ... 223. 222. 221.]\n",
      "   [175. 193. 215. ... 226. 223. 220.]\n",
      "   ...\n",
      "   [186. 199. 210. ... 195. 189. 189.]\n",
      "   [185. 194. 204. ... 212. 205. 200.]\n",
      "   [190. 194. 201. ... 223. 216. 209.]]\n",
      "\n",
      "  [[212. 217. 221. ... 191. 182. 186.]\n",
      "   [202. 208. 217. ... 193. 186. 186.]\n",
      "   [197. 201. 210. ... 196. 191. 187.]\n",
      "   ...\n",
      "   [228. 225. 221. ... 190. 177. 182.]\n",
      "   [228. 225. 222. ... 186. 173. 177.]\n",
      "   [227. 227. 226. ... 181. 169. 171.]]]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert train and test labels into 'int64' type\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_images)\n\u001b[0;32m----> 5\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m test_labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# # Convert train and test labels into 'int64' type\n",
    "\n",
    "\n",
    "# train_labels = train_labels.astype('int64')\n",
    "# test_labels = test_labels.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0, Max: 255.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize the images data\n",
    "\n",
    "print('Min: {}, Max: {}'.format(train_images.min(), train_images.max()))\n",
    "\n",
    "train_images /= 255.0\n",
    "test_images /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of train and test labels\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "train_labels = lb.fit_transform(train_labels)\n",
    "test_labels = lb.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3, 73257]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split train data into train and validation sets\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ftg-mNQeF3m2/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/ftg-mNQeF3m2/lib/python3.9/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.virtualenvs/ftg-mNQeF3m2/lib/python3.9/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3, 73257]"
     ]
    }
   ],
   "source": [
    "# Split train data into train and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels,\n",
    "                                                  test_size=0.15, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get more robust results out of our model, we are going to augment the images in the dataset, by randomly rotating them, zooming them in and out, shifting them up and down (**IMPORTANT NOTE:** It is best that we do not shift them horizontally, since there are also distracting digits in the images), shifting their channels and shearing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=8,\n",
    "                             zoom_range=[0.95, 1.05],\n",
    "                             height_shift_range=0.10,\n",
    "                             shear_range=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine a good learning rate for the optimizer of our model (here, we use the AMSGrad variant of the Adam optimizer), we set a callback in an auxillary model which will gradually increase the learning rate of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define auxillary model\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "aux_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                           activation='relu',\n",
    "                           input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),    \n",
    "    keras.layers.Dense(10,  activation='softmax')\n",
    "])\n",
    "\n",
    "lr_schedule = keras.callbacks.LearningRateScheduler(\n",
    "              lambda epoch: 1e-4 * 10**(epoch / 10))\n",
    "optimizer = keras.optimizers.Adam(lr=1e-4, amsgrad=True)\n",
    "aux_model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model in order to determine best learning rate\n",
    "\n",
    "history = aux_model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n",
    "                              epochs=30, validation_data=(X_val, y_val),\n",
    "                              callbacks=[lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Learning Rate vs. Loss\n",
    "\n",
    "plt.semilogx(history.history['lr'], history.history['loss'])\n",
    "plt.axis([1e-4, 1e-1, 0, 4])\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss follows a very specific trajectory: a rapid drop followed by a relatively flat line which shoots back up after a certain point. Thus, it is better to choose a learning rate in the region where the loss is stable; a reasonable choice would be **lr = 0.01** (or 1e-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define actual model\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                           activation='relu',\n",
    "                           input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', \n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same', \n",
    "                           activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, (3, 3), padding='same',\n",
    "                        activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),    \n",
    "    keras.layers.Dense(10,  activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=8)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3, amsgrad=True)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "                   '/kaggle/working/best_cnn.h5', \n",
    "                   save_best_only=True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model in order to make predictions\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n",
    "                              epochs=70, validation_data=(X_val, y_val),\n",
    "                              callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate train and validation accuracies and losses\n",
    "\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize epochs vs. train and validation accuracies and losses\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Epochs vs. Training and Validation Accuracy')\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Epochs vs. Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing how the algorithm converged, we can now evaluate the model's performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "test_loss, test_acc = model.evaluate(x=test_images, y=test_labels, verbose=0)\n",
    "\n",
    "print('Test accuracy is: {:0.4f} \\nTest loss is: {:0.4f}'.\n",
    "      format(test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! This result implies that our error ranges from ~4% to ~4.4% and we should have in mind that the errors reported by state-of-the-art models and methods range from 4.9% to 1.02% ([reference](https://benchmarks.ai/svhn)). Of course, there is room for quite a bit of tuning in order to improve performance such as:\n",
    "- Change the way the images are transformed in the augmentation process.\n",
    "- Change the architecture of our model by adding extra blocks, changing the kernel sizes, making it deeper, etc.\n",
    "- Train multiple CNNs and make ensemble predictions.\n",
    "- Use some of the extra data which can be found along with the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we can present some nice and useful visualizations which make us understand better how our Convolutional Neural Network actually works. It is my belief that the two following visualizations are the most helpful:\n",
    "\n",
    "- The **Confusion Matrix** of the model on the training data, so as to get a sense of how it performs on each class label and how the misclassifications are distributed.\n",
    "- The **Feature Maps** for a random input image, so as to get a sense of how our model learns the features in each convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and apply inverse transformation to the labels\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "y_pred = lb.inverse_transform(y_pred, lb.classes_)\n",
    "y_train = lb.inverse_transform(y_train, lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(y_train, y_pred, labels=lb.classes_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(matrix, annot=True, cmap='Greens', fmt='d', ax=ax)\n",
    "plt.title('Confusion Matrix for training dataset')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the training data consists mostly of 0s, 1s and 2s (in a descending order), while labels '5' up to '9' are underepresented. Furthermore, the confusion matrix can show us the particular problematic cases of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the errors in the plots\n",
    "\n",
    "np.seterr(all='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get convolutional layers\n",
    "\n",
    "layers = [model.get_layer('conv2d_1'), \n",
    "          model.get_layer('conv2d_2'),\n",
    "          model.get_layer('conv2d_3'),\n",
    "          model.get_layer('conv2d_4'),\n",
    "          model.get_layer('conv2d_5'),\n",
    "          model.get_layer('conv2d_6')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model which gives the outputs of the layers\n",
    "\n",
    "layer_outputs = [layer.output for layer in layers]\n",
    "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the names of the layers\n",
    "\n",
    "layer_names = []\n",
    "for layer in layers:\n",
    "    layer_names.append(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which will plot the convolutional filters\n",
    "\n",
    "def plot_convolutional_filters(img):\n",
    "    \n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    activations = activation_model.predict(img)\n",
    "    images_per_row = 9\n",
    "    \n",
    "    for layer_name, layer_activation in zip(layer_names, activations): \n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        for col in range(n_cols): \n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0,\n",
    "                                                 :, :,\n",
    "                                                 col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size,\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_train[42500]\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convolutional_filters(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we can see the main idea behind Convolutional Neural Networks, i.e. that as we go deeper into the layers, higher-level features are learned by our models. At first, we can easily understand what is going on but gradually, we find it difficult to keep up with the model's learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, we have trained a Convolutional Neural Network to recognize the digits in the Street View House Numbers dataset (Format 2). In particular, we have performed some minimal preprocessing of the data, we have augmented the data in various ways, we have created an auxillary model in order to find which learning rate we should choose for our optimizer and finally, we have trained the final CNN and evaluated it on the test images data. Furthermore, we have provided two useful visualizations (confusion matrix and feature maps) so as get a sense of how our model actually works and not view it as just a black-box process. Finally, it should be noted that there is quite a bit of room for tuning and different architectures so as to improve the accuracy of the model; nonetheless, our results are pretty good given the simplicity of our approach.\n",
    "\n",
    "I hope you enjoyed this simple kernel and find it useful, either as a reference for your own projects or as an introduction to the complete process of training CNNs. Also, **PLEASE DO NOT FORGET TO UPVOTE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftg-mNQeF3m2",
   "language": "python",
   "name": "ftg-mnqef3m2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
